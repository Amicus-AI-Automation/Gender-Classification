{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(name):\n",
    "    name = name.lower()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # BASIC FEATURES (keep existing logic)    \n",
    "    features[\"first_letter\"] = name[0]\n",
    "    features[\"last_letter\"] = name[-1]\n",
    "    features[\"name_length\"] = len(name)\n",
    "\n",
    "    features[\"ends_with_vowel\"] = name[-1] in \"aeiou\"\n",
    "    features[\"ends_with_consonant\"] = name[-1] not in \"aeiou\"\n",
    "    features[\"starts_with_vowel\"] = name[0] in \"aeiou\"\n",
    "\n",
    "    features[\"last_2\"] = name[-2:] if len(name) >= 2 else name\n",
    "    features[\"last_3\"] = name[-3:] if len(name) >= 3 else name\n",
    "    \n",
    "    # INDIAN-SPECIFIC SUFFIX FEATURES    \n",
    "    female_suffixes = (\n",
    "        \"a\", \"i\", \"aa\", \"ya\", \"ni\", \"ika\", \"ini\", \"thi\", \"thra\",\n",
    "        \"mitha\", \"shree\", \"rani\", \"latha\", \"vani\", \"sri\"\n",
    "    )\n",
    "\n",
    "    male_suffixes = (\n",
    "        \"n\", \"an\", \"esh\", \"raj\", \"shan\", \"kar\", \"deep\", \"dev\",\n",
    "        \"kumar\", \"th\", \"ran\", \"eshan\"\n",
    "    )\n",
    "\n",
    "    for suf in female_suffixes:\n",
    "        features[f\"ends_with_female_{suf}\"] = name.endswith(suf)\n",
    "\n",
    "    for suf in male_suffixes:\n",
    "        features[f\"ends_with_male_{suf}\"] = name.endswith(suf)\n",
    "\n",
    "   \n",
    "    # VOWEL STATISTICS    \n",
    "    features[\"vowel_count\"] = sum(1 for c in name if c in \"aeiou\")\n",
    "    features[\"vowel_ratio\"] = features[\"vowel_count\"] / len(name)    \n",
    "   \n",
    "    # Using boundary markers improves learning\n",
    "    padded = f\"<{name}>\"\n",
    "\n",
    "    # 2-grams, 3-grams, 4-grams\n",
    "    for n in (2, 3, 4):\n",
    "        for i in range(len(padded) - n + 1):\n",
    "            gram = padded[i:i+n]\n",
    "            features[f\"char_{n}gram_{gram}\"] = True\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "f9100dc9-851f-4b12-90a5-151c4fc076ab",
    "_uuid": "46c91d5a820e15798031e4e7f99bd7e26948395f",
    "execution": {
     "iopub.execute_input": "2026-01-19T10:28:33.997926Z",
     "iopub.status.busy": "2026-01-19T10:28:33.997399Z",
     "iopub.status.idle": "2026-01-19T10:28:34.014638Z",
     "shell.execute_reply": "2026-01-19T10:28:34.013576Z",
     "shell.execute_reply.started": "2026-01-19T10:28:33.997886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Vibhish', 'male'),\n",
       " ('Anmay', 'male'),\n",
       " ('Kraanti', 'male'),\n",
       " ('Yuveena', 'female'),\n",
       " ('Sivahulan', 'male'),\n",
       " ('Kogulakannan', 'male'),\n",
       " ('Harika', 'female'),\n",
       " ('Mishtu', 'female'),\n",
       " ('Maharudra', 'male'),\n",
       " ('Vani', 'female')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the CSV file\n",
    "# Replace 'names.csv' with your actual CSV file path\n",
    "df = pd.read_csv(\"sample_indian_names.csv\")\n",
    "\n",
    "# Map numeric gender labels to text labels\n",
    "# 0 -> male, 1 -> female\n",
    "df[\"Gender\"] = df[\"Gender\"].map({0: \"male\", 1: \"female\"})\n",
    "\n",
    "# Convert to list of (name, gender) tuples\n",
    "labeled_names = list(zip(df[\"Name\"], df[\"Gender\"]))\n",
    "\n",
    "# Shuffle the combined data\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "labeled_names[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 53982\n",
      "Training+Validation samples: 48584\n",
      "Test samples: 5398\n",
      "Fold 1 Accuracy: 0.9266\n",
      "Fold 2 Accuracy: 0.9297\n",
      "Fold 3 Accuracy: 0.9296\n",
      "Fold 4 Accuracy: 0.9324\n",
      "Fold 5 Accuracy: 0.9298\n",
      "\n",
      "Average CV Accuracy: 0.9296270414017647\n",
      "Final Test Accuracy: 0.9260837347165617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature extraction\n",
    "featuresets = [(gender_features(name), gender) for (name, gender) in labeled_names]\n",
    "\n",
    "# Shuffle once before splitting\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "\n",
    "# Train / Test split (90% train, 10% test)\n",
    "TOTAL_SIZE = len(featuresets)\n",
    "TEST_SIZE = int(0.10 * TOTAL_SIZE)\n",
    "\n",
    "test_set = featuresets[:TEST_SIZE]\n",
    "train_val_set = featuresets[TEST_SIZE:]\n",
    "\n",
    "print(\"Total samples:\", TOTAL_SIZE)\n",
    "print(\"Training+Validation samples:\", len(train_val_set))\n",
    "print(\"Test samples:\", len(test_set))\n",
    "\n",
    "\n",
    "# K-Fold Cross Validation on Training data\n",
    "K = 5\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_set), 1):\n",
    "\n",
    "    # Split folds\n",
    "    train_fold = [train_val_set[i] for i in train_idx]\n",
    "    val_fold = [train_val_set[i] for i in val_idx]\n",
    "\n",
    "    X_train = [f for f, y in train_fold]\n",
    "    y_train = [y for f, y in train_fold]\n",
    "\n",
    "    X_val = [f for f, y in val_fold]\n",
    "    y_val = [y for f, y in val_fold]\n",
    "\n",
    "    # Vectorize features\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "    # Logistic Regression model\n",
    "    classifier = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        solver=\"liblinear\",\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "    # Validate\n",
    "    y_pred = classifier.predict(X_val_vec)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    cv_accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nAverage CV Accuracy:\", sum(cv_accuracies) / K)\n",
    "\n",
    "\n",
    "# Final model training on full training data\n",
    "\n",
    "X_train_full = [f for f, y in train_val_set]\n",
    "y_train_full = [y for f, y in train_val_set]\n",
    "\n",
    "final_vectorizer = DictVectorizer(sparse=True)\n",
    "X_train_full_vec = final_vectorizer.fit_transform(X_train_full)\n",
    "\n",
    "final_classifier = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "final_classifier.fit(X_train_full_vec, y_train_full)\n",
    "\n",
    "\n",
    "# Final evaluation on Test set\n",
    "X_test = [f for f, y in test_set]\n",
    "y_test = [y for f, y in test_set]\n",
    "\n",
    "X_test_vec = final_vectorizer.transform(X_test)\n",
    "y_test_pred = final_classifier.predict(X_test_vec)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Final Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create directory to store model artifacts\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Save trained model and vectorizer\n",
    "joblib.dump(final_classifier, \"model/gender_model.joblib\")\n",
    "joblib.dump(final_vectorizer, \"model/vectorizer.joblib\")\n",
    "\n",
    "print(\"Model and vectorizer saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Name: Vibhish, Label: male\n",
      "2. Name: Anmay, Label: male\n",
      "3. Name: Kraanti, Label: male\n",
      "4. Name: Yuveena, Label: female\n",
      "5. Name: Sivahulan, Label: male\n",
      "6. Name: Kogulakannan, Label: male\n",
      "7. Name: Harika, Label: female\n",
      "8. Name: Mishtu, Label: female\n",
      "9. Name: Maharudra, Label: male\n",
      "10. Name: Vani, Label: female\n",
      "11. Name: Saptanshu, Label: male\n",
      "12. Name: Kesavalu, Label: male\n",
      "13. Name: Yuganthini, Label: female\n",
      "14. Name: Jeyagowry, Label: female\n",
      "15. Name: Darmendran, Label: male\n",
      "16. Name: Jathush, Label: male\n",
      "17. Name: Jananthya, Label: female\n",
      "18. Name: Sivamurugan, Label: male\n",
      "19. Name: Letchumanan, Label: male\n",
      "20. Name: Sasivarman, Label: male\n"
     ]
    }
   ],
   "source": [
    "# Create test data with original names preserved\n",
    "TEST_SIZE = len(test_set)\n",
    "\n",
    "test_rows = labeled_names[:TEST_SIZE]\n",
    "\n",
    "for i in range(20):\n",
    "    name, gender = test_rows[i]\n",
    "    print(f\"{i+1}. Name: {name}, Label: {gender}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset saved as test_dataset.csv\n",
      "Total test samples: 5398\n"
     ]
    }
   ],
   "source": [
    "test_data = labeled_names[:TEST_SIZE]\n",
    "\n",
    "# Convert test data to DataFrame\n",
    "test_df = pd.DataFrame(test_data, columns=[\"Name\", \"Gender\"])\n",
    "\n",
    "# Optional: convert labels back to numeric if needed\n",
    "# male -> 0, female -> 1\n",
    "test_df[\"Gender\"] = test_df[\"Gender\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "# Save to CSV\n",
    "test_df.to_csv(\"test_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Test dataset saved as test_dataset.csv\")\n",
    "print(\"Total test samples:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9260837347165617\n",
      "\n",
      "Confusion Matrix Counts:\n",
      "Actual Male -> Predicted Male: 2618\n",
      "Actual Male -> Predicted Female: 253\n",
      "Actual Female -> Predicted Male: 146\n",
      "Actual Female -> Predicted Female: 2381\n",
      "\n",
      "Sample Predictions (Actual vs Predicted):\n",
      "1. Actual: female, Predicted: female\n",
      "2. Actual: female, Predicted: female\n",
      "3. Actual: male, Predicted: male\n",
      "4. Actual: male, Predicted: male\n",
      "5. Actual: female, Predicted: female\n",
      "6. Actual: female, Predicted: female\n",
      "7. Actual: female, Predicted: female\n",
      "8. Actual: female, Predicted: female\n",
      "9. Actual: female, Predicted: female\n",
      "10. Actual: female, Predicted: female\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for features, actual_gender in test_set:\n",
    "    X_test.append(features)\n",
    "    y_test.append(actual_gender)\n",
    "\n",
    "# Vectorize test features\n",
    "X_test_vec = final_vectorizer.transform(X_test)\n",
    "\n",
    "# Predict\n",
    "y_pred = final_classifier.predict(X_test_vec)\n",
    "y_true = y_test\n",
    "\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Confusion matrix (manual)\n",
    "confusion = Counter()\n",
    "\n",
    "for actual, predicted in zip(y_true, y_pred):\n",
    "    confusion[(actual, predicted)] += 1\n",
    "\n",
    "print(\"\\nConfusion Matrix Counts:\")\n",
    "print(\"Actual Male -> Predicted Male:\", confusion[(\"male\", \"male\")])\n",
    "print(\"Actual Male -> Predicted Female:\", confusion[(\"male\", \"female\")])\n",
    "print(\"Actual Female -> Predicted Male:\", confusion[(\"female\", \"male\")])\n",
    "print(\"Actual Female -> Predicted Female:\", confusion[(\"female\", \"female\")])\n",
    "\n",
    "\n",
    "# Show few sample predictions\n",
    "print(\"\\nSample Predictions (Actual vs Predicted):\")\n",
    "for i in range(10):\n",
    "    name_features, actual = test_set[i]\n",
    "    pred = final_classifier.predict(\n",
    "        final_vectorizer.transform([name_features])\n",
    "    )[0]\n",
    "\n",
    "    print(f\"{i+1}. Actual: {actual}, Predicted: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions (Name | Actual | Predicted):\n",
      "1. Name: Vibhish, Actual: male, Predicted: male\n",
      "2. Name: Anmay, Actual: male, Predicted: male\n",
      "3. Name: Kraanti, Actual: male, Predicted: male\n",
      "4. Name: Yuveena, Actual: female, Predicted: female\n",
      "5. Name: Sivahulan, Actual: male, Predicted: male\n",
      "6. Name: Kogulakannan, Actual: male, Predicted: male\n",
      "7. Name: Harika, Actual: female, Predicted: female\n",
      "8. Name: Mishtu, Actual: female, Predicted: female\n",
      "9. Name: Maharudra, Actual: male, Predicted: male\n",
      "10. Name: Vani, Actual: female, Predicted: female\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample Predictions (Name | Actual | Predicted):\")\n",
    "\n",
    "for i in range(10):\n",
    "    name, actual = test_data[i]          # name preserved\n",
    "    features = gender_features(name)     # dict features\n",
    "\n",
    "    # Vectorize (VERY IMPORTANT)\n",
    "    features_vec = final_vectorizer.transform([features])\n",
    "\n",
    "    # Predict\n",
    "    predicted = final_classifier.predict(features_vec)[0]\n",
    "\n",
    "    print(f\"{i+1}. Name: {name}, Actual: {actual}, Predicted: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender_with_unisex(name, threshold=0.80, margin=0.65):\n",
    "    features = gender_features(name)\n",
    "    features_vec = final_vectorizer.transform([features])\n",
    "\n",
    "    probs = final_classifier.predict_proba(features_vec)[0]\n",
    "    classes = [str(c) for c in final_classifier.classes_]\n",
    "\n",
    "    prob_dict = {cls: float(prob) for cls, prob in zip(classes, probs)}\n",
    "\n",
    "    sorted_probs = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_label, top_prob = sorted_probs[0]\n",
    "    second_label, second_prob = sorted_probs[1]\n",
    "\n",
    "    if top_prob < threshold or (top_prob - second_prob) < margin:\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"prediction\": \"Common\",\n",
    "            \"confidence\": top_prob,\n",
    "            \"probabilities\": prob_dict\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"prediction\": top_label,\n",
    "        \"confidence\": top_prob,\n",
    "        \"probabilities\": prob_dict\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR set to: d:\\customapps\\ECAT\\Gender-Classification\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()   # Notebook-safe base directory\n",
    "print(\"BASE_DIR set to:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_feedback(name, model_prediction, human_verdict, confidence):\n",
    "    \"\"\"\n",
    "    Stores human feedback in feedback.csv\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(BASE_DIR, \"feedback-identification.csv\")\n",
    "\n",
    "    row = pd.DataFrame(\n",
    "        [[name, model_prediction, human_verdict, confidence]],\n",
    "        columns=[\n",
    "            \"Name\",\n",
    "            \"Model_Prediction\",\n",
    "            \"Human_Verdict\",\n",
    "            \"Confidence\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        row.to_csv(file_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        row.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Feedback saved at: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback saved at: d:\\customapps\\ECAT\\Gender-Classification\\feedback-identification.csv\n"
     ]
    }
   ],
   "source": [
    "# Name input (single source of truth)\n",
    "name_input = \"juspreet\"\n",
    "\n",
    "# Model prediction\n",
    "result = predict_gender_with_unisex(name_input)\n",
    "\n",
    "# Human label\n",
    "human_verdict = \"female\"\n",
    "\n",
    "# Store feedback using the same name\n",
    "store_feedback(\n",
    "    name=name_input,\n",
    "    model_prediction=result[\"prediction\"],\n",
    "    human_verdict=human_verdict,\n",
    "    confidence=result[\"confidence\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'juspreet', 'prediction': 'Common', 'confidence': 0.5826166369063538, 'probabilities': {'female': 0.4173833630936462, 'male': 0.5826166369063538}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the Retained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded v1 model and vectorizer\n",
      "\n",
      "Feedback samples loaded: 103\n",
      "      Name Model_Prediction Human_Verdict  Confidence\n",
      "0  swapnil             male        common    0.999485\n",
      "1     Ravi           Common          male    0.807898\n",
      "2  Nandini           female        female    0.921275\n",
      "3    Payal           female        female    0.926518\n",
      "4    Sonal             male        female    0.995755\n",
      "\n",
      "Label distribution after oversampling:\n",
      "Counter({'male': 854, 'female': 532, 'common': 56})\n",
      "\n",
      "Weights copied from v1 → initializing v2\n",
      "\n",
      "v2 model retrained using human feedback\n",
      "\n",
      " v2 model saved successfully\n",
      "v1 model path: d:\\customapps\\ECAT\\Gender-Classification\\model\\gender_model.joblib\n",
      "v2 model path: d:\\customapps\\ECAT\\Gender-Classification\\model\\gender_model_v2.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swapnil_singh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Retrain Gender Model using Human Feedback\n",
    "# (v1 → v2 with warm start & versioning)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Versioned paths\n",
    "MODEL_V1_PATH = os.path.join(MODEL_DIR, \"gender_model.joblib\")\n",
    "MODEL_V2_PATH = os.path.join(MODEL_DIR, \"gender_model_v2.joblib\")\n",
    "VECTORIZER_PATH = os.path.join(MODEL_DIR, \"vectorizer.joblib\")\n",
    "FEEDBACK_PATH = os.path.join(BASE_DIR, \"feedback-identification.csv\")\n",
    "\n",
    "# Load v1 artifacts\n",
    "final_classifier = joblib.load(MODEL_V1_PATH)\n",
    "final_vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "print(\"Loaded v1 model and vectorizer\")\n",
    "\n",
    "# Feature extractor (same as training)\n",
    "def gender_features(name):\n",
    "    name = name.lower()\n",
    "    return {\n",
    "        \"last_letter\": name[-1],\n",
    "        \"first_letter\": name[0],\n",
    "        \"name_length\": len(name),\n",
    "        \"suffix_2\": name[-2:],\n",
    "        \"suffix_3\": name[-3:],\n",
    "        \"prefix_2\": name[:2],\n",
    "        \"prefix_3\": name[:3]\n",
    "    }\n",
    "\n",
    "\n",
    "# Load feedback data\n",
    "feedback_df = pd.read_csv(FEEDBACK_PATH)\n",
    "\n",
    "print(\"\\nFeedback samples loaded:\", len(feedback_df))\n",
    "print(feedback_df.head())\n",
    "\n",
    "# Use HUMAN verdict as source of truth\n",
    "feedback_df[\"Final_Label\"] = feedback_df[\"Human_Verdict\"]\n",
    "\n",
    "# Build training data from feedback\n",
    "# (Human feedback is more important → oversample)\n",
    "feedback_features = []\n",
    "feedback_labels = []\n",
    "\n",
    "OVERSAMPLE_FACTOR = 14   # human verdict priority\n",
    "\n",
    "for _, row in feedback_df.iterrows():\n",
    "    for _ in range(OVERSAMPLE_FACTOR):\n",
    "        feedback_features.append(gender_features(row[\"Name\"]))\n",
    "        feedback_labels.append(row[\"Final_Label\"])\n",
    "\n",
    "print(\"\\nLabel distribution after oversampling:\")\n",
    "print(Counter(feedback_labels))\n",
    "\n",
    "# Vectorize\n",
    "X_feedback_vec = final_vectorizer.transform(feedback_features)\n",
    "y_feedback = feedback_labels\n",
    "\n",
    "\n",
    "# CREATE v2 MODEL (COPY v1 WEIGHTS → WARM START)\n",
    "retrained_classifier = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "# Copy learned parameters from v1\n",
    "retrained_classifier.classes_ = final_classifier.classes_\n",
    "retrained_classifier.coef_ = np.copy(final_classifier.coef_)\n",
    "retrained_classifier.intercept_ = np.copy(final_classifier.intercept_)\n",
    "retrained_classifier.n_features_in_ = final_classifier.n_features_in_\n",
    "print(\"\\nWeights copied from v1 → initializing v2\")\n",
    "\n",
    "# Retrain model (continues learning)\n",
    "retrained_classifier.fit(X_feedback_vec, y_feedback)\n",
    "print(\"\\nv2 model retrained using human feedback\")\n",
    "\n",
    "# Save v2 model (DO NOT overwrite v1)\n",
    "joblib.dump(retrained_classifier, MODEL_V2_PATH)\n",
    "\n",
    "print(\"\\n v2 model saved successfully\")\n",
    "print(\"v1 model path:\", MODEL_V1_PATH)\n",
    "print(\"v2 model path:\", MODEL_V2_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded v1 and v2 models\n",
      "\n",
      "Test Set Accuracy:\n",
      "v1 (Before Feedback): 0.9261\n",
      "v2 (After Feedback):  0.704\n",
      "\n",
      "Feedback Data Accuracy (Human Verdict):\n",
      "v1 (Before Feedback): 0.5631\n",
      "v2 (After Feedback):  0.8155\n",
      "\n",
      "Accuracy Improvement Summary:\n",
      "Test Set Change     : -0.2221\n",
      "Feedback Set Change : 0.2524\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Comparison: v1 vs v2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ----------------\n",
    "# Paths\n",
    "# ----------------\n",
    "BASE_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model\")\n",
    "\n",
    "MODEL_V1_PATH = os.path.join(MODEL_DIR, \"gender_model.joblib\")\n",
    "MODEL_V2_PATH = os.path.join(MODEL_DIR, \"gender_model_v2.joblib\")\n",
    "VECTORIZER_PATH = os.path.join(MODEL_DIR, \"vectorizer.joblib\")\n",
    "FEEDBACK_PATH = os.path.join(BASE_DIR, \"feedback-identification.csv\")\n",
    "\n",
    "# Load models & vectorizer\n",
    "model_v1 = joblib.load(MODEL_V1_PATH)\n",
    "model_v2 = joblib.load(MODEL_V2_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "print(\"Loaded v1 and v2 models\")\n",
    "\n",
    "# Feature extractor (same as training)\n",
    "def gender_features(name):\n",
    "    name = name.lower()\n",
    "    return {\n",
    "        \"last_letter\": name[-1],\n",
    "        \"first_letter\": name[0],\n",
    "        \"name_length\": len(name),\n",
    "        \"suffix_2\": name[-2:],\n",
    "        \"suffix_3\": name[-3:],\n",
    "        \"prefix_2\": name[:2],\n",
    "        \"prefix_3\": name[:3]\n",
    "    }\n",
    "\n",
    "# 1️⃣ Accuracy on ORIGINAL TEST SET (Before Feedback)\n",
    "X_test = [features for features, _ in test_set]\n",
    "y_test = [label for _, label in test_set]\n",
    "\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "y_test_pred_v1 = model_v1.predict(X_test_vec)\n",
    "y_test_pred_v2 = model_v2.predict(X_test_vec)\n",
    "\n",
    "acc_test_v1 = accuracy_score(y_test, y_test_pred_v1)\n",
    "acc_test_v2 = accuracy_score(y_test, y_test_pred_v2)\n",
    "\n",
    "print(\"\\nTest Set Accuracy:\")\n",
    "print(\"v1 (Before Feedback):\", round(acc_test_v1, 4))\n",
    "print(\"v2 (After Feedback): \", round(acc_test_v2, 4))\n",
    "\n",
    "\n",
    "# Accuracy on FEEDBACK DATA (Human Truth)\n",
    "feedback_df = pd.read_csv(FEEDBACK_PATH)\n",
    "\n",
    "X_feedback = [gender_features(name) for name in feedback_df[\"Name\"]]\n",
    "y_feedback = feedback_df[\"Human_Verdict\"].tolist()\n",
    "\n",
    "X_feedback_vec = vectorizer.transform(X_feedback)\n",
    "\n",
    "y_feedback_pred_v1 = model_v1.predict(X_feedback_vec)\n",
    "y_feedback_pred_v2 = model_v2.predict(X_feedback_vec)\n",
    "\n",
    "acc_feedback_v1 = accuracy_score(y_feedback, y_feedback_pred_v1)\n",
    "acc_feedback_v2 = accuracy_score(y_feedback, y_feedback_pred_v2)\n",
    "\n",
    "print(\"\\nFeedback Data Accuracy (Human Verdict):\")\n",
    "print(\"v1 (Before Feedback):\", round(acc_feedback_v1, 4))\n",
    "print(\"v2 (After Feedback): \", round(acc_feedback_v2, 4))\n",
    "\n",
    "# Improvement Summary\n",
    "\n",
    "print(\"\\nAccuracy Improvement Summary:\")\n",
    "print(\"Test Set Change     :\", round(acc_test_v2 - acc_test_v1, 4))\n",
    "print(\"Feedback Set Change :\", round(acc_feedback_v2 - acc_feedback_v1, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1975,
     "sourceId": 3392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
